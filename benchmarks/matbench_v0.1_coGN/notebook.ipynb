{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc19213c-23a8-45ce-ac6d-93b7f496341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras as ks\n",
    "\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from crystalgnns.crystal_preprocessing.crystal_preprocessors import (RadiusAsymmetricUnitCell,\n",
    "                                                                     KNNAsymmetricUnitCell,\n",
    "                                                                     VoronoiAsymmetricUnitCell)\n",
    "from crystalgnns.graph_dataset_tools.graph_tuple import HDFGraphTuple, GraphTuple\n",
    "from crystalgnns.graph_dataset_tools.model_preprocessors import GNPreprocessor\n",
    "from crystalgnns.datasets.matbench_datasets import MatbenchDataset\n",
    "from crystalgnns.models.graph_network.graph_network import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e6b85-ec93-495c-a0bb-c7412e0450bf",
   "metadata": {},
   "source": [
    "## Functions for Training/Evaluation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04d474-56b6-4a75-ab65-85677604b38b",
   "metadata": {},
   "source": [
    "Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97d378-5e14-47cc-85a0-bfc774becd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_preprocessor(use_multiplicity = True, use_voronoi_area = False,\n",
    "                           use_line_graph = True, line_graph_direction=[1,1]):\n",
    "    model_inputs = [GNPreprocessor.Inputs.offset,\n",
    "        GNPreprocessor.Inputs.atomic_number,\n",
    "        GNPreprocessor.Inputs.edge_indices]\n",
    "    if use_multiplicity:\n",
    "        model_inputs.append(GNPreprocessor.Inputs.multiplicity)\n",
    "    if use_voronoi_area:\n",
    "        model_inputs.append(GNPreprocessor.Inputs.voronoi_ridge_area)\n",
    "    if use_line_graph:\n",
    "        model_inputs.append(GNPreprocessor.Inputs.line_graph_edge_indices)\n",
    "    model_preprocessor = GNPreprocessor(inputs=model_inputs,\n",
    "                                        line_graph_directions=line_graph_direction)\n",
    "    return model_preprocessor\n",
    "\n",
    "def get_lr_scheduler(dataset_size, batch_size, epochs, lr_start=0.0005, lr_stop=1e-5):\n",
    "    steps_per_epoch = dataset_size / batch_size\n",
    "    num_steps = epochs * steps_per_epoch\n",
    "    scheduler = ks.optimizers.schedules.PolynomialDecay(initial_learning_rate=lr_start,\n",
    "                                                        decay_steps=num_steps,\n",
    "                                                        end_learning_rate=lr_stop)\n",
    "    return scheduler\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3cce03-7b76-4d6e-a03f-8182b14a7d20",
   "metadata": {},
   "source": [
    "Define training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da40a3-e3c9-4cf9-b88d-fe6e7a9d3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_procedure(model_cfg, crystal_preprocessor, matbench_datasets_subset, use_multiplicity = True,\n",
    "                    use_voronoi_area = False, use_line_graph = True, line_graph_direction = [1,1],\n",
    "                    dataset_cache=Path('./dataset_cache'), results_cache=Path('./results'),\n",
    "                    use_scaler=True, epochs=800, batch_size=64):\n",
    "    # MatBench training procedure\n",
    "    dataset_cache = Path(dataset_cache)\n",
    "    results_cache = Path(results_cache)\n",
    "    \n",
    "    model_preprocessor = get_model_preprocessor(use_multiplicity=use_multiplicity,\n",
    "                                                use_voronoi_area=use_voronoi_area,\n",
    "                                                use_line_graph=use_line_graph,\n",
    "                                                line_graph_direction=line_graph_direction)\n",
    "\n",
    "    mb_dataset_cache = MatbenchDataset(dataset_cache)\n",
    "    \n",
    "    mb = MatbenchBenchmark(subset=matbench_datasets_subset, autoload=False)\n",
    "    for idx_task, task in enumerate(mb.tasks):\n",
    "\n",
    "        # Create/Access cached file for preprocessed crystals\n",
    "        preprocessed_crystals_file = mb_dataset_cache.get_dataset_file(task.dataset_name, crystal_preprocessor)\n",
    "        f = h5py.File(preprocessed_crystals_file, 'r')\n",
    "        preprocessed_crystals = HDFGraphTuple(f)\n",
    "\n",
    "        # Mapping from unique crystal ids in dataset to index of the cached file\n",
    "        id_index_mapping = {id_.decode():i for i, id_ in enumerate(preprocessed_crystals.graph_attributes['dataset_id'][:])}\n",
    "        def get_graphs(inputs):\n",
    "            idxs = [id_index_mapping[id_] for id_ in inputs.index]\n",
    "            return preprocessed_crystals[idxs]\n",
    "\n",
    "        task.load()\n",
    "        for fold in task.folds:\n",
    "            intermediate_results_ = results_cache / task.dataset_name / str(fold)\n",
    "            if intermediate_results_.exists():\n",
    "                print('Load intermediate results')\n",
    "                predictions = np.load(str(intermediate_results_ / 'predictions.npy'))\n",
    "                history_dict = json.load(open(intermediate_results_ / 'history.json', 'r'))\n",
    "            else:\n",
    "                # Get training data with target values\n",
    "                train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "                train_graphs = get_graphs(train_inputs)\n",
    "                train_graphs.graph_attributes['label'] = train_outputs.to_numpy()\n",
    "                train_graphs.graph_attribute_names.append('label')\n",
    "                x_train, y_train = model_preprocessor.to_ragged_tensors(train_graphs)\n",
    "                y_train = np.expand_dims(y_train,-1)\n",
    "\n",
    "                if use_scaler and task.metadata[\"task_type\"] != \"classification\":\n",
    "                    scaler = StandardScaler()\n",
    "                    y_train = scaler.fit_transform(y_train)\n",
    "                \n",
    "                if task.metadata[\"task_type\"] == \"classification\":\n",
    "                    loss = ks.losses.BinaryCrossentropy(from_logits=True)\n",
    "                    metrics = [ks.metrics.AUC(from_logits=True)]\n",
    "                else:\n",
    "                    loss = ks.losses.MeanAbsoluteError()\n",
    "                    metrics = [ks.losses.MeanAbsoluteError(), ks.losses.MeanSquaredError()]\n",
    "                    \n",
    "                model = get_model(model_cfg.input_block_cfg,\n",
    "                     [model_cfg.processing_block_cfg]* model_cfg.depth,\n",
    "                     model_cfg.output_block_cfg,\n",
    "                     multiplicity=use_multiplicity,\n",
    "                     voronoi_ridge_area=use_voronoi_area,\n",
    "                     line_graph=use_line_graph)\n",
    "                scheduler = get_lr_scheduler(y_train.shape[0], batch_size, epochs,\n",
    "                                             lr_start=0.0005, lr_stop=1e-5)\n",
    "                optimizer = ks.optimizers.Adam(learning_rate=scheduler)\n",
    "                model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "                \n",
    "                # Train model\n",
    "                start = datetime.now()\n",
    "                history = model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs)\n",
    "                duration_in_seconds = (datetime.now() - start).total_seconds()\n",
    "\n",
    "                # Get test data\n",
    "                test_inputs = task.get_test_data(fold, include_target=False)\n",
    "                test_graphs = get_graphs(test_inputs)\n",
    "                test_graphs.graph_attributes['label'] = np.zeros(len(test_inputs)) # dummy values\n",
    "                test_graphs.graph_attribute_names.append('label')\n",
    "                x_test, _ = model_preprocessor.to_ragged_tensors(test_graphs)\n",
    "\n",
    "                # Predict\n",
    "                predictions = model.predict(x_test)\n",
    "                \n",
    "                if use_scaler and task.metadata[\"task_type\"] != \"classification\":\n",
    "                    predictions = scaler.inverse_transform(predictions)\n",
    "                if predictions.shape[-1] == 1:\n",
    "                    predictions = np.squeeze(predictions, axis=-1)\n",
    "\n",
    "                intermediate_results_.mkdir(parents=True)\n",
    "                history_dict = {k: np.array(v).tolist() for k,v in history.history.items()}\n",
    "                history_dict['training_time'] = duration_in_seconds\n",
    "                np.save(str(intermediate_results_ / 'predictions.npy'), predictions)\n",
    "                json.dump(history_dict, open(intermediate_results_ / 'history.json', 'w'))\n",
    "                model.save_weights(str(intermediate_results_ / 'weights.h5'))\n",
    "\n",
    "            task.record(fold, predictions, params={\n",
    "                'input_block': model_cfg.input_block_cfg,\n",
    "                'processing_blocks': [model_cfg.processing_block_cfg]* model_cfg.depth,\n",
    "                'output_block': model_cfg.output_block_cfg})\n",
    "        f.close()\n",
    "    return mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec353004-6876-4dae-b43d-e96000b61f59",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbbee0-b63d-4dcd-bd96-5cf1965e4d90",
   "metadata": {},
   "source": [
    "Define configuration manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03911525-8563-492c-924a-72de47e03316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "units = 128\n",
    "depth = 5\n",
    "\n",
    "input_block_cfg = {'node_size': units,\n",
    " 'edge_size': units,\n",
    " 'atomic_mass': True,\n",
    " 'atomic_radius': True,\n",
    " 'electronegativity': True,\n",
    " 'ionization_energy': True,\n",
    " 'oxidation_states': True,\n",
    " 'edge_embedding_args': {'bins_distance': 32,\n",
    "  'max_distance': 8.0,\n",
    "  'distance_log_base': 1.0,\n",
    "  'bins_voronoi_area': None,\n",
    "  'max_voronoi_area': None}}\n",
    "\n",
    "processing_block_cfg = {'edge_mlp': {'units': [units]*5,\n",
    "  'activation': ['swish']*5},\n",
    " 'node_mlp': {'units': [units]*1,\n",
    "  'activation': ['swish']*1},\n",
    " 'global_mlp': None,\n",
    " 'nested_blocks_cfgs': None,\n",
    " 'aggregate_edges_local': 'sum',\n",
    " 'aggregate_edges_global': None,\n",
    " 'aggregate_nodes': None,\n",
    " 'return_updated_edges': False,\n",
    " 'return_updated_nodes': True,\n",
    " 'return_updated_globals': True,\n",
    " 'edge_attention_mlp_local': {'units': [32, 1],\n",
    "  'activation': ['swish', 'swish']},\n",
    " 'edge_attention_mlp_global': {'units': [32, 1],\n",
    "  'activation': ['swish', 'swish']},\n",
    " 'node_attention_mlp': {'units': [32, 1], 'activation': ['swish', 'swish']},\n",
    " 'edge_gate': None,\n",
    " 'node_gate': None,\n",
    " 'global_gate': None,\n",
    " 'residual_node_update': True,\n",
    " 'residual_edge_update': False,\n",
    " 'residual_global_update': False,\n",
    " 'update_edges_input': [True, True, True, False],\n",
    " 'update_nodes_input': [True, False, False],\n",
    " 'update_global_input': [False, True, False],\n",
    " 'multiplicity_readout': False}\n",
    "\n",
    "output_block_cfg = {'edge_mlp': None,\n",
    " 'node_mlp': None,\n",
    " 'global_mlp': {'units': [1],\n",
    "  'activation': ['linear']},\n",
    " 'nested_blocks_cfgs': None,\n",
    " 'aggregate_edges_local': 'sum',\n",
    " 'aggregate_edges_global': None,\n",
    " 'aggregate_nodes': 'mean',\n",
    " 'return_updated_edges': False,\n",
    " 'return_updated_nodes': True,\n",
    " 'return_updated_globals': True,\n",
    " 'edge_attention_mlp_local': {'units': [32, 1],\n",
    "  'activation': ['swish', 'swish']},\n",
    " 'edge_attention_mlp_global': {'units': [32, 1],\n",
    "  'activation': ['swish', 'swish']},\n",
    " 'node_attention_mlp': {'units': [32, 1], 'activation': ['swish', 'swish']},\n",
    " 'edge_gate': None,\n",
    " 'node_gate': None,\n",
    " 'global_gate': None,\n",
    " 'residual_node_update': False,\n",
    " 'residual_edge_update': False,\n",
    " 'residual_global_update': False,\n",
    " 'update_edges_input': [True, True, True, False],\n",
    " 'update_nodes_input': [True, False, False],\n",
    " 'update_global_input': [False, True, False],\n",
    " 'multiplicity_readout': True}\n",
    "\n",
    "model_cfg = {\n",
    "    'units': units,\n",
    "    'depth': depth,\n",
    "    'input_block_cfg': input_block_cfg,\n",
    "    'processing_block_cfg': processing_block_cfg,\n",
    "    'output_block_cfg': output_block_cfg\n",
    "}\n",
    "\n",
    "# Make keys accessible via dot notation\n",
    "model_cfg = dotdict(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332eae37-8ad3-4060-b40d-33777f0d077d",
   "metadata": {},
   "source": [
    "... or load from seperate python file/module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a04dab-c23c-4abb-8f66-3d915ef8e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coGN_config as model_cfg\n",
    "# import coNGN_config as model_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa666d5-f637-46e7-af37-aeeedfb9c7eb",
   "metadata": {},
   "source": [
    "## Run Evaluation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c86ab-2844-4c84-9173-3e031480e4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matbench_datasets_subset = [\"matbench_dielectric\", \"matbench_phonons\", \"matbench_jdft2d\"]\n",
    "mb = train_procedure(model_cfg, KNNAsymmetricUnitCell(24),\n",
    "                         matbench_datasets_subset,\n",
    "                         use_multiplicity = True,\n",
    "                         use_voronoi_area = False, use_line_graph = False, line_graph_direction = [1,1],\n",
    "                         dataset_cache=Path('./dataset_cache'),\n",
    "                         results_cache=Path('./results'),\n",
    "                         use_scaler=True,\n",
    "                         epochs=8,\n",
    "                         batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_venv",
   "language": "python",
   "name": "ma_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
